# FASTQ Parser - High-Performance FASTQ File Parser in Rust

A blazingly fast fastq parser written in Rust with SIMD vectorization, memory-mapped I/O, and parallel processing capabilities.

## Table of Contents

- [Installation](#installation)
- [Quick Start](#quick-start)
- [Core API](#core-api)
  - [Reading & Parsing](#reading--parsing)
  - [Parallel Processing](#parallel-processing)
  - [SIMD Operations](#simd-operations)
  - [Memory-Mapped Files](#memory-mapped-files)
  - [Record Processing](#record-processing)
  - [Error Handling](#error-handling)
  - [Buffer Management](#buffer-management)
- [Advanced Usage](#advanced-usage)
  - [Zero-Copy Parsing](#zero-copy-parsing)
  - [Compression Support](#compression-support)
  - [Streaming Mode](#streaming-mode)
- [Examples](#examples)
- [Performance](#performance)
- [API Reference](#api-reference)
- [Development](#development)
- [License](#license)

## Installation

Add to your `Cargo.toml`:

```toml
[dependencies]
fastq-parser = "0.1.0"
```

Or using `pixi` for environment management:

```bash
# Install pixi
curl -fsSL https://pixi.sh/install.sh | bash

# Install dependencies
pixi install

# Build the project
pixi run cargo build --release
```

## Quick Start

```rust
use fastq_parser::{FastqReader, Result};

fn main() -> Result<()> {
    // Parse FASTQ file
    let reader = FastqReader::from_path("input.fastq")?;
    
    // Iterate over records
    for record in reader.into_records() {
        let record = record?;
        println!("ID: {}", std::str::from_utf8(&record.id)?);
        println!("Sequence length: {}", record.seq.len());
    }
    
    Ok(())
}
```

### Features

- **Core Parsing**: Blazingly fast FASTQ parsing with SIMD optimization
- **Paired-End Support**: Synchronous iteration through R1/R2 file pairs
- **Advanced Filtering**: Filter by length, quality, N-content, and ID patterns
- **Format Conversion**: FASTQ to FASTA conversion with metadata preservation
- **Random Access**: Index-based O(1) lookup of specific reads
- **Barcode/UMI Processing**: Demultiplex and deduplicate molecular barcodes
- **Quality Metrics**: Per-position quality distributions and duplicate detection

## Core API

### Reading & Parsing

**[`FastqReader`](./docs/reader.md)** - Main entry point for parsing FASTQ files

```rust
use fastq_parser::{FastqReader, Parser, Result};

// Basic file reading
let reader = FastqReader::from_path("input.fastq")?;

// Parse gzipped files
let reader = FastqReader::from_path("input.fastq.gz")?;

// Parse from bytes
let data = std::fs::read("input.fastq")?;
let reader = FastqReader::from_bytes(data)?;

// Streaming iteration (constant memory)
for record in reader.into_records() {
    let r = record?;
    // Process one record at a time
}
```
[ðŸ“– Full Reader Documentation](./docs/reader.md)

### Parallel Processing

**[`ParallelParser`](./docs/parallel.md)** - Multi-threaded parsing for large files

```rust
use fastq_parser::parallel::ParallelParser;

// Create parallel parser
let parser = ParallelParser::new(data);

// Process records in parallel with callback
parser.parse_with_callback(|record| {
    // Process each record
    println!("Processing: {}", std::str::from_utf8(&record.id)?);
})?;

// Automatic chunk boundary detection
let results = parser.parse_parallel()?;
```
[ðŸ“– Full Parallel Documentation](./docs/parallel.md)

### SIMD Operations

**[`simd`](./docs/simd.md)** - Vectorized operations using AVX2

```rust
use fastq_parser::simd;

// Find all newline positions (uses AVX2 if available)
let positions = simd::find_newlines(data);

// Validate ASCII content
let is_valid = simd::validate_ascii(data);

// Count specific characters
let count = simd::count_chars(data, b'A');

// Vectorized quality score processing
let avg_quality = simd::average_quality(quality_scores);
```
[ðŸ“– Full SIMD Documentation](./docs/simd.md)

### Memory-Mapped Files

**[`mmap`](./docs/mmap.md)** - Efficient handling of large files

```rust
use fastq_parser::{FastqReader, MmapOptions};

// Automatic memory mapping for large files
let reader = FastqReader::from_path("huge.fastq")?;

// Manual memory-mapped reading
let mmap_reader = FastqReader::from_mmap("huge.fastq")?;

// Configure mmap options
let options = MmapOptions::new()
    .prefetch(true)
    .sequential_access(true);
```
[ðŸ“– Full Memory-Mapping Documentation](./docs/mmap.md)

### Record Processing

**[`FastqRecord`](./docs/record.md)** - Core data structure for FASTQ entries

```rust
use fastq_parser::{FastqRecord, QualityEncoding};

// Access record components
let id = std::str::from_utf8(&record.id)?;
let sequence = &record.seq;
let quality = &record.qual;

// Quality score conversion
let phred_scores = record.to_phred_scores(QualityEncoding::Sanger)?;

// Record validation
let is_valid = record.validate()?;
```
[ðŸ“– Full Record Documentation](./docs/record.md)

### Error Handling

**[`Error`](./docs/error.md)** - Comprehensive error types

```rust
use fastq_parser::{Result, Error};

match reader.parse() {
    Ok(records) => process(records),
    Err(Error::InvalidFormat(msg)) => eprintln!("Format error: {}", msg),
    Err(Error::Io(e)) => eprintln!("I/O error: {}", e),
    Err(e) => eprintln!("Other error: {}", e),
}
```
[ðŸ“– Full Error Documentation](./docs/error.md)

### Buffer Management

**[`BufferPool`](./docs/buffer.md)** - Efficient memory reuse

```rust
use fastq_parser::buffer::BufferPool;

// Create buffer pool for streaming
let pool = BufferPool::new(capacity);

// Reuse buffers across iterations
let buffer = pool.get();
// ... use buffer ...
pool.return(buffer);
```
[ðŸ“– Full Buffer Documentation](./docs/buffer.md)

## Advanced Usage

### Builder Patterns

Configure readers and parsers with builder patterns:

```rust
use fastq_parser::{FastqReaderBuilder, ParserBuilder};

// Configure reader with custom settings
let reader = FastqReaderBuilder::new()
    .buffer_size(64 * 1024)
    .validate_paired(true)
    .strict_mode(false)
    .build("input.fastq")?;

// Configure parser with specific options
let parser = ParserBuilder::new()
    .max_record_size(10_000)
    .allow_truncated(false)
    .quality_encoding(QualityEncoding::Sanger)
    .build(data)?;
```

### Advanced Streaming

Process large files with advanced streaming options:

```rust
use fastq_parser::{AsyncStreamingReader, ChunkedStreamer};

// Asynchronous streaming for concurrent I/O
let async_reader = AsyncStreamingReader::new("huge.fastq")?;
while let Some(record) = async_reader.next_async().await? {
    // Process with async I/O
}

// Chunked streaming with configurable chunk size
let chunked = ChunkedStreamer::new("huge.fastq")?
    .chunk_size(1024 * 1024)  // 1MB chunks
    .prefetch(true);

for chunk in chunked.chunks() {
    let records = chunk?;
    // Process chunk of records
}
```

### Zero-Copy Parsing

Minimize allocations with zero-copy techniques:

```rust
// Parser returns views into original data
let parser = Parser::new();
let records = parser.parse_zero_copy(&data)?;

// No string allocations - work with byte slices
for record in records {
    // record.id, record.seq, record.qual are all &[u8]
}
```

### Compression Support

Native support for gzip-compressed files:

```rust
// Automatic detection based on file extension
let reader = FastqReader::from_path("input.fastq.gz")?;

// Explicit compressed reading
let reader = FastqReader::from_gzip("compressed.gz")?;

// Streaming decompression
for record in reader.iter_compressed()? {
    // Process decompressed records on-the-fly
}
```

### Streaming Mode

Process files of any size with constant memory:

```rust
// Stream large files without loading into memory
let mut reader = FastqReader::streaming("huge.fastq")?;

while let Some(record) = reader.next_record()? {
    // Process one record at a time
    // Memory usage remains constant
}
```

### Paired-End Read Processing

Handle paired-end sequencing data:

```rust
use fastq_parser::{PairedEndReader, InterleavedReader};

// Process R1/R2 file pairs
let paired_reader = PairedEndReader::from_paths("R1.fastq", "R2.fastq")?;

for pair_result in paired_reader.into_paired_records() {
    let (r1, r2) = pair_result?;
    // Process paired reads together
}

// Process interleaved FASTQ files (R1/R2 alternating)
let interleaved = InterleavedReader::from_path("interleaved.fastq")?;

for pair_result in interleaved.into_paired_records() {
    let (r1, r2) = pair_result?;
    // Process alternating paired reads
}
```

### Advanced Filtering

Filter reads based on multiple criteria:

```rust
use fastq_parser::{AdvancedFilter, QualityFilter, AdapterTrimmer};

// Content-based filtering
let filter = AdvancedFilter::new()
    .min_length(50)
    .max_length(300)
    .max_n_ratio(0.1)
    .max_n_count(10);

// Apply filter to records
if filter.filter(&record) {
    // Process passing reads
}

// Quality-based filtering and trimming
let quality_filter = QualityFilter::new()
    .min_quality(20.0)
    .min_length(50)
    .trim_quality(Some(20));

// Filter and trim low-quality ends
if let Some(trimmed) = quality_filter.trim(&record) {
    // Process trimmed read
}

// Adapter trimming
let adapter_trimmer = AdapterTrimmer::new(vec![b"AGATCGGAAGAG".to_vec()])
    .min_overlap(6)
    .error_rate(0.1);

let trimmed_record = adapter_trimmer.trim(&record);
```

### Barcode Processing

Demultiplex and process barcoded reads:

```rust
use fastq_parser::{BarcodeConfig, Demultiplexer, UmiDeduplicator, BarcodeCorrector};

// Configure barcode extraction
let config = BarcodeConfig::new(0, 8)  // 8bp barcode at position 0
    .with_umi(8, 10)  // 10bp UMI at position 8
    .max_mismatches(1);

// Demultiplex by barcode
let demux = Demultiplexer::new(config, barcode_map);
let stats = demux.demultiplex_to_files(records, "output/", "sample")?;

// UMI deduplication
let deduplicator = UmiDeduplicator::new()
    .min_quality(20)
    .max_distance(1);

let unique_reads = deduplicator.deduplicate(records)?;

// Barcode error correction
let corrector = BarcodeCorrector::new(known_barcodes)
    .max_distance(2);

let corrected_barcode = corrector.correct(observed_barcode);
```

### Index-Based Random Access

Create and use an index for O(1) read lookup:

```rust
use fastq_parser::{FastqIndex, IndexedReader, RandomAccessReader};

// Build index
let index = FastqIndex::build("reads.fastq")?;
index.save("reads.fqi")?;

// Use index for random access
let reader = IndexedReader::from_paths("reads.fastq", "reads.fqi")?;
let record = reader.get_record("READ_ID_12345");  // Returns Option<Record>

// Alternative: RandomAccessReader for position-based access
let random_reader = RandomAccessReader::new("reads.fastq")?;
let record_at_pos = random_reader.get_by_position(1000)?;  // Get 1000th record
let records_range = random_reader.get_range(100, 200)?;  // Get records 100-200
```

### Format Conversion and Writing

Convert between formats and write filtered subsets:

```rust
use fastq_parser::{FastqWriter, FastaWriter, FormatConverter, SubsetExtractor};

// Write FASTQ records (with automatic gzip if .gz extension)
let mut writer = FastqWriter::to_file("output.fastq.gz")?;
writer.write_record(&record)?;

// Write FASTA format
let mut fasta_writer = FastaWriter::to_file("output.fasta")?;
fasta_writer.write_record(&record)?;

// Batch format conversion
let count = FormatConverter::fastq_to_fasta("input.fastq", "output.fasta")?;

// Extract subset based on ID list
let ids = vec!["READ1", "READ2", "READ3"];
let extracted = SubsetExtractor::extract_by_ids("input.fastq", "subset.fastq", &ids)?;

// Filter and write in one operation
let (total, passed) = FormatConverter::filter_and_write(
    "input.fastq",
    "filtered.fastq",
    |record| record.len() >= 100 && record.mean_quality() >= 20.0
)?;
```

### Quality Metrics and Analysis

Compute comprehensive quality metrics:

```rust
use fastq_parser::{QualityMetrics, ErrorDetector, QualityPlotter};

// Collect quality metrics
let mut metrics = QualityMetrics::new();
for record in reader.into_records() {
    let mut rec = record?;
    metrics.update(&mut rec);
}
metrics.finalize();

// Get position-specific quality statistics
let position_stats = metrics.position_quality_stats();
let duplicate_rate = metrics.duplicate_rate();

// Detect sequencing errors
let detector = ErrorDetector::new()
    .kmer_size(5)
    .error_threshold(0.001);

let error_kmers = detector.detect_errors(&records)?;

// Generate quality plots (if plotting enabled)
let plotter = QualityPlotter::new();
plotter.plot_quality_distribution(&metrics, "quality_plot.png")?;

## Examples

### Complete Processing Pipeline

```rust
use fastq_parser::*;

fn process_fastq(path: &str) -> Result<()> {
    // 1. Create reader with optimal settings
    let reader = FastqReader::from_path(path)?;
    
    // 2. Use parallel processing for large files
    if std::fs::metadata(path)?.len() > 100_000_000 {
        let parser = ParallelParser::from_file(path)?;
        parser.parse_with_callback(|record| {
            // Process in parallel
            analyze_record(&record)
        })?;
    } else {
        // 3. Stream smaller files
        for record in reader.into_records() {
            analyze_record(&record?)?;
        }
    }
    
    Ok(())
}

fn analyze_record(record: &FastqRecord) -> Result<()> {
    // Calculate metrics
    let gc_content = calculate_gc_content(&record.seq);
    let avg_quality = calculate_average_quality(&record.qual)?;
    
    // Filter by quality
    if avg_quality >= 30.0 {
        // Process high-quality reads
    }
    
    Ok(())
}
```

### Run Examples

```bash
# Basic usage
pixi run cargo run --example basic_usage

# Parallel processing demonstration
pixi run cargo run --example parallel

# Streaming large files
pixi run cargo run --example streaming

# SIMD operations showcase
pixi run cargo run --example simd_demo

# Advanced features (filtering, barcodes, metrics)
pixi run cargo run --example advanced_features
```

## Performance

### Benchmarks

Run benchmarks:

```bash
pixi run cargo bench
```

### Performance Characteristics

| File Size | Method | Throughput | Memory |
|-----------|--------|------------|--------|
| <10 MB | In-memory | 3+ GB/s | Low |
| 10-100 MB | Memory-mapped | 3+ GB/s | Medium |
| >100 MB | Parallel + mmap | 3+ GB/s Ã— cores | Medium |
| Any size | Streaming | 1-2 GB/s | Constant |
| Gzipped | Decompression | ~500 MB/s | Low |

### Optimizations

1. **SIMD Vectorization**: AVX2 instructions for 32-byte parallel processing
2. **Zero-Copy Parsing**: Return views into original data
3. **Memory Mapping**: Direct file access without heap allocation
4. **Parallel Chunking**: Intelligent splitting at record boundaries
5. **Buffer Pooling**: Reuse allocations in streaming mode
6. **Link-Time Optimization**: LTO enabled for release builds

### Performance Metrics

Based on our benchmarks:
- **3+ GB/s** throughput on uncompressed files
- **~450ms** to parse 5.6M records (comparable to needletail)
- **Linear scaling** with multiple CPU cores
- **Constant memory** streaming for files of any size

## API Reference

Complete API documentation is organized by module:

### ðŸ“š [Complete API Documentation](./docs/api-reference.md)

### Core Modules

| Module | Description | Documentation |
|--------|-------------|---------------|
| **[parser](./docs/parser.md)** | Core parsing logic and algorithms | [ðŸ“– Docs](./docs/parser.md) |
| **[reader](./docs/reader.md)** | File I/O and streaming | [ðŸ“– Docs](./docs/reader.md) |
| **[parallel](./docs/parallel.md)** | Multi-threaded processing | [ðŸ“– Docs](./docs/parallel.md) |
| **[simd](./docs/simd.md)** | SIMD vectorization | [ðŸ“– Docs](./docs/simd.md) |
| **[record](./docs/record.md)** | FASTQ record structures | [ðŸ“– Docs](./docs/record.md) |
| **[buffer](./docs/buffer.md)** | Buffer management | [ðŸ“– Docs](./docs/buffer.md) |
| **[error](./docs/error.md)** | Error handling | [ðŸ“– Docs](./docs/error.md) |
| **[mmap](./docs/mmap.md)** | Memory-mapped I/O | [ðŸ“– Docs](./docs/mmap.md) |
| **[paired](./docs/paired.md)** | Paired-end read handling | [ðŸ“– Docs](./docs/paired.md) |
| **[writer](./docs/writer.md)** | FASTQ/FASTA writing & conversion | [ðŸ“– Docs](./docs/writer.md) |
| **[filter](./docs/filter.md)** | Advanced read filtering | [ðŸ“– Docs](./docs/filter.md) |
| **[index](./docs/index.md)** | Index-based random access | [ðŸ“– Docs](./docs/index.md) |
| **[barcode](./docs/barcode.md)** | Barcode/UMI processing | [ðŸ“– Docs](./docs/barcode.md) |
| **[metrics](./docs/metrics.md)** | Quality metrics & analysis | [ðŸ“– Docs](./docs/metrics.md) |

### Quick Links to Key Types

- [`FastqReader`](./docs/reader.md#fastqreader-struct) - Main entry point for parsing
- [`FastqRecord`](./docs/record.md#fastqrecord-struct) - Single FASTQ entry
- [`Parser`](./docs/parser.md#parser-struct) - Core parsing engine
- [`ParallelParser`](./docs/parallel.md#parallelparser-struct) - Multi-threaded parser
- [`BufferPool`](./docs/buffer.md#bufferpool-struct) - Memory management
- [`Error`](./docs/error.md#error-enum) - Error types

## Development

### Setup

Using pixi for environment management:

```bash
# Install dependencies
pixi install

# Run tests
pixi run cargo test

# Run specific test
pixi run cargo test test_parallel_parser

# Check code
pixi run cargo check
pixi run cargo clippy

# Format code
pixi run cargo fmt

# Build documentation
pixi run cargo doc --open

# Run benchmarks
pixi run cargo bench
```

### Project Structure

```
fastq-parser/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ lib.rs         # Public API exports
â”‚   â”œâ”€â”€ main.rs        # CLI application
â”‚   â”œâ”€â”€ parser.rs      # Core parsing logic
â”‚   â”œâ”€â”€ simd.rs        # SIMD optimizations
â”‚   â”œâ”€â”€ parallel.rs    # Parallel processing
â”‚   â”œâ”€â”€ reader.rs      # File I/O abstractions
â”‚   â”œâ”€â”€ buffer.rs      # Buffer management
â”‚   â”œâ”€â”€ record.rs      # Record structures
â”‚   â”œâ”€â”€ error.rs       # Error handling
â”‚   â”œâ”€â”€ stream.rs      # Streaming support
â”‚   â”œâ”€â”€ filter.rs      # Advanced filtering
â”‚   â”œâ”€â”€ paired.rs      # Paired-end reads
â”‚   â”œâ”€â”€ writer.rs      # Output & conversion
â”‚   â”œâ”€â”€ index.rs       # Random access index
â”‚   â”œâ”€â”€ barcode.rs     # Barcode/UMI processing
â”‚   â””â”€â”€ metrics.rs     # Quality metrics
â”œâ”€â”€ benches/
â”‚   â””â”€â”€ parser_bench.rs # Performance benchmarks
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ integration_tests.rs
â”‚   â””â”€â”€ new_features_test.rs
â”œâ”€â”€ examples/          # Example usage
â”‚   â””â”€â”€ advanced_features.rs
â”œâ”€â”€ docs/             # Module documentation
â”œâ”€â”€ testdata/         # Test FASTQ files
â”œâ”€â”€ Cargo.toml
â”œâ”€â”€ pixi.toml         # Pixi environment config
â””â”€â”€ README.md
```

### Contributing

Contributions are welcome! Please ensure:
- All tests pass: `pixi run cargo test`
- Code follows Rust formatting: `pixi run cargo fmt`
- No clippy warnings: `pixi run cargo clippy`
- New features include tests
- Performance improvements include benchmarks

## License

MIT License

## Acknowledgments

This parser was inspired by:
- [needletail](https://github.com/onecodex/needletail) - Fast FASTX parsing in Rust
- [seq_io](https://github.com/markschl/seq_io) - FASTA/FASTQ parsing
- [kseq.h](https://github.com/lh3/seqtk) - Classic C implementation
- [biofast benchmarks](https://github.com/lh3/biofast) - Bioinformatics benchmarking